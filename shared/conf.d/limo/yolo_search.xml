<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3AYOLO%26id_list%3D%26start%3D0%26max_results%3D2" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:YOLO&amp;id_list=&amp;start=0&amp;max_results=2</title>
  <id>http://arxiv.org/api/LfUiosNM0nCz8zo99UM118HggFI</id>
  <updated>2017-06-11T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">7</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">2</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/1705.09587v1</id>
    <updated>2017-05-26T14:07:41Z</updated>
    <published>2017-05-26T14:07:41Z</published>
    <title>Enhancement of SSD by concatenating feature maps for object detection</title>
    <summary>  We propose an object detection method that improves the accuracy of the
conventional SSD (Single Shot Multibox Detector), which is one of the top
object detection algorithms in both aspects of accuracy and speed. The
performance of a deep network is known to be improved as the number of feature
maps increases. However, it is difficult to improve the performance by simply
raising the number of feature maps. In this paper, we propose and analyze how
to use feature maps effectively to improve the performance of the conventional
SSD. The enhanced performance was obtained by changing the structure close to
the classifier network, rather than growing layers close to the input data,
e.g., by replacing VGGNet with ResNet. The proposed network is suitable for
sharing the weights in the classifier networks, by which property, the training
can be faster with better generalization power. For the Pascal VOC 2007 test
set trained with VOC 2007 and VOC 2012 training sets, the proposed network with
the input size of 300 x 300 achieved 78.5% mAP (mean average precision) at the
speed of 35.0 FPS (frame per second), while the network with a 512 x 512 sized
input achieved 80.8% mAP at 16.6 FPS using Nvidia Titan X GPU. The proposed
network shows state-of-the-art mAP, which is better than those of the
conventional SSD, YOLO, Faster-RCNN and RFCN. Also, it is faster than
Faster-RCNN and RFCN.
</summary>
    <author>
      <name>Jisoo Jeong</name>
    </author>
    <author>
      <name>Hyojin Park</name>
    </author>
    <author>
      <name>Nojun Kwak</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05922v1</id>
    <updated>2017-05-16T21:05:49Z</updated>
    <published>2017-05-16T21:05:49Z</published>
    <title>LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object
  Detection in Embedded Systems</title>
    <summary>  Deep convolutional Neural Networks (CNN) are the state-of-the-art performers
for object detection task. It is well known that object detection requires more
computation and memory than image classification. Thus the consolidation of a
CNN-based object detection for an embedded system is more challenging. In this
work, we propose LCDet, a fully-convolutional neural network for generic object
detection that aims to work in embedded systems. We design and develop an
end-to-end TensorFlow(TF)-based model. Additionally, we employ 8-bit
quantization on the learned weights. We use face detection as a use case. Our
TF-Slim based network can predict different faces of different shapes and sizes
in a single forward pass. Our experimental results show that the proposed
method achieves comparative accuracy comparing with state-of-the-art CNN-based
face detection methods, while reducing the model size by 3x and memory-BW by
~4x comparing with one of the best real-time CNN-based object detector such as
YOLO. TF 8-bit quantized model provides additional 4x memory reduction while
keeping the accuracy as good as the floating point model. The proposed model
thus becomes amenable for embedded implementations.
</summary>
    <author>
      <name>Subarna Tripathi</name>
    </author>
    <author>
      <name>Gokce Dane</name>
    </author>
    <author>
      <name>Byeongkeun Kang</name>
    </author>
    <author>
      <name>Vasudev Bhaskaran</name>
    </author>
    <author>
      <name>Truong Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Embedded Vision Workshop in CVPR</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>